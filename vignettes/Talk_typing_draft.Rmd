---
title             : "Typing and Talking: draft in progress"
shorttitle        : "Crosstalk during typing"

author: 
  - name          : "Matthew J. C. Crump"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Brooklyn College of CUNY, 2900 Bedford Avenue, Brooklyn, NY, 11210"
    email         : "mcrump@brooklyn.cuny.edu"
  - name          : "Nicholaus Brosowsky"
    affiliation   : "2"
  - name          : "Lawrence Behmer"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Brooklyn College of the City University of New York"
  - id            : "2"
    institution   : "Graduate Center of the City University of New York"


author_note: |
  [https://github.com/CrumpLab/TalkTyping](https://github.com/CrumpLab/TalkTyping) is the github repository for this project containing the data, and source code for compiling compiling this paper in R using papaja.
  
abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "Typing, Language production"
wordcount         : "X"

bibliography      : ["r-references.bib","TypingTalking.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
vignette: >
  %\VignetteIndexEntry{TalkTyping}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r load_packages, include = FALSE}
library(papaja)
library(TalkTyping)
library(ggpubr)
require(ggplot2)
require(Crump)
require(dplyr)
require(knitr)
require(xtable)
```

```{r analysis_preferences, echo=FALSE}
# Seed for random number generation
set.seed(42)
```

Language production processes turn ideas into actions. For example, people use utterances and gestures to convey semantic meaning during conversation; to preserve ideas in different mediums, such as hand-writing, typewriting, and dictation; and to plan and guide cognition and behavior in general. In Vygotszky's () developmental tradition, children first acquire connections between actions and outcomes in the world around them with the words spoken to them by their caregivers, and then internalize the same language system as a tool to regulate their own cognition and behavior. In adulthood, people continue to talk out loud and silently to themselves using inner speech, and such self-directed language production is assumed to continue to regulate many aspects of cognition and behavior (). For example, in memory, the phonological loop transfers to-be-remembered information into long-term memory through short-term rehearsal [@BaddeleyWorkingmemory1974].  The act of speaking out loud itself improves future retrieval, as shown by the production effect [@MacLeodproductioneffectDelineation2010]; and, the process of naming may guide category learning by drawing attention toward diagnostic features (Lupyan,). Verbal recoding can facilitate learning and recall of lengthy sequences by casting elements into chunks with recallable names [@Millermagicalnumberseven1956]. In the completion of everyday tasks, @BotvinickDoingschemahierarchies2004 argue that verbally coded instructions together with recurrent associative learning of real-world action sequences provide the control necessary for accomplishing routine tasks like making coffee or tea. Among these and other uses, a crowning achievement of language production is the ability to control the serial order of utterances to convey meaning in highly complex and flexible ways. In the present work, we are interested how the verbal process of language production for speech may or may not be recruited to facilitate manual control of fingers for sequencing keystrokes during typewriting. In particular, we investigate the general idea that the sequence production processes enabling fluent typing are controlled by common word level linguistic codes used during speech.

We review three perspectives from the typing, language production, and sequence production literatures that all suggest an important role for verbal codes to mediate keystroke production in typing. Typing is assumed to be controlled by hierarchically nested control loops [@LoganHierarchicalcontrolcognitive2011]. The outer loop relies on language comprehension and production to turn ideas into words and sentences. Word level-representation are then sent as goals to an inner loop that controls individual finger movements to individual keystrokes for each letter in a word. The two loops are informationally encapsulated modules [@Fodormodularitymindessay1983], and the outer loop does not know the details of how the inner loop accomplishes keystrokes [@Loganlefthanddoesn2009a;@Liuyouknowwhere2010]. However, the two loops must nevertheless be coordinated to accomplish accurate typing, presumably by intermediate word-level representations. For example, in @RumelhartSimulatingskilledtypist1982 typing model, words cause parallel activation of letter units in a response scheduling buffer, which then controls individual keystrokes for all letters in the word. That model focused on copy-typing, and assumed that word representations are activated by word-recognition processes operating during reading of to-be-copied text. In composition typing, word representations could be activated by overt or covert language production processes, and their activation would presumably have the same consequence of activating constituent letters for typing.
	
People are capable of overt and covert speech, which may be mediated by a common language production process. For example, according to the flexible abstraction hypothesis (), the selection of individual phonemes during covert or overt speech is controlled by a common process with relatively abstract phonemic codes. These abstract codes retain deep semantic and lexical features, but lack many of the articulatory features necessary for speaking out loud. As a result, inner speech may rely on these more abstract codes, which explains evidence that errors in overt speech can be driven more by lexical than phonemic similarity (). At the same time, these abstract codes can be used flexibly and further modified by articulatory processes that fill in necessary additional features to achieve overt speech. Although the flexible abstraction hypothesis is concerned mainly with speech overt and covert speech production, it is worth considering whether it extends to nonverbal language production such as typing. For example, the same abstract verbal codes mediating inner speech may be co-opted and further specified by motor processes controlling finger movements to achieve accurate and fluid keystroke production.
Sequence production more generally may also be mediated by verbal codes that trigger a plan-based mode of action control distinct from a more automatic stimulus-based mode of control (). For example, a stimulus-based mode of control refers to actions triggered in an automatic, reflex-like manner in response to well-learned cues from the environment. Whereas, a plan-based mode of control refers to goal-driven cognitive processes that set and supervise motor programs for complex sequencing of actions. Evidence for a distinction between control modes comes from the serial reaction time task [SRT, @NissenAttentionalrequirementslearning1987]. The major finding from SRT tasks is that subjects learn to respond faster to targets that are presented in repeating or probabilistic sequences compared to random sequences, even if they are unable to explicitly describe the sequential regularities. At the same time, whether reaction times are sensitive to the frequency of particular targets or first-order transitions (e.g., repetitions or alternations) appears to depend on whether subjects possess explicit knowledge about sequential regularities in the task. For example, in a task where alternations were three times more likely than repetitions, subjects who were not aware of the manipulation showed faster responses to the more frequent alternations than repetitions, whereas subjects who were aware of the manipulation were equally fast for alternations and repetitions. One interpretation is that explicit knowledge of the transition probabilities allowed subjects to adopt a control-based mode, enabling action plans to mediate responses rather than more automatic frequency sensitive learning processes. Using a similar task, Tubau, Hommel, & LÃ³pez-Moliner (2007) further showed that subjects were more likely to adopt a plan-based mode (as evidenced by the elimination of the alternation benefit) when visual/verbal targets were used and when responses where followed by sounds, and plan-based control was disrupted when auditory noise was introduced. They suggested that use of phonemic codes in general may strongly influence subjects to adopt a plan-based vs. stimulus-based mode of action control.

Several lines of evidence suggest that verbal codes for speech production participate in manual control over keystrokes during typing. Typing speed is fastest for words and decreases as letter strings approach random texts (). This effect of string structure could simply reflect familiarity and prior experience with typing words compared to random letter sequences, but could also be driven by string pronounceability. For example, typing speed is also influenced by syllabic structure, with faster interkeystroke times for letters within versus between syllables in a word. Word-level representations may also have a special status in activating all constituent letters in parallel compared to non-words which do not activate all letters in a sequence (). Finally, lexical structure at the level of bigrams appears to influence the spatial precision with which typists locate keys during typing (). This evidence is consistent with the idea that typists use verbal codes in a straightforward manner during typing, by concurrently using their inner voice (and perhaps sometimes speaking out loud) to say the same words they are typing.

Other work looking at dual-task interference in typing suggests that fast and accurate typing may not necessarily depend on using verbal codes. Protection from dual-task interference is often interpreted as evidence that is a skill is highly automatized, and typing in general appears to be highly automatized. For example, typing speed in a dual task-condition to identify auditory tones with a foot-pedal was only 4ms slower than normal typing. Remarkably, Shaffer (1975) showed that typing speed was mostly unaffected under concurrent task demands to recite an unrelated nursery rhyme while typing. However, typing speed was slowed substantially when typists copied spoken words while simultaneously reading unrelated visually presented text. And, these data were collected only from a single skilled typist, and whether the result is generalizable remains unclear. On the one hand, if the same verbal codes used for speech are used for typing, it is surprising that a typist could easily type one set of words while saying another set of words out loud without interference. On the other hand, interference appears to depend on the details of how verbal codes are used, as interference was observed when typists attempted to read unrelated text while copying spoken words.

The purpose of the present work is to clarify whether and how typists use verbal codes during typing. One possibility is that verbal codes for speech do not participate directly in the control of keystrokes for typing. On this view, typing could be considered a highly automatized and modularized skill that runs largely independently from other language production processes. Although speaking and typing may often go hand in hand, as many typists report silently voicing the words they are typing while they produce text (see Experiment 1), overt or covert speech production during typing could be carried out independently and in parallel with the motor movements required for typing. Another general possibility is that typists co-opt existing speech production processes during typing, so that normal typing is deeply connected at some level to a common language production process. We test these two general ideas in a series of related typing tasks that manipulate what typists say while they are typing. A central empirical aim was to tax concurrent speech production processes with reciting relevant or irrelevant content during copy-typing. In this way, we could establish boundary conditions for observing interference (or the lack thereof) from speech production on keystroke production.

In Experiment 1a and 1b we had typists speak the words or letters they were typing either silently or out loud. This experiment tested the idea that typists use verbal codes at the word rather than letter level during typing. The remaining experiments investigated the extent to which typing performance can be impaired by disrupting normal speech production. Experiment 2 measured copy-typing performance across several verbal suppression conditions where typists were instructed to repeat words or letter (unrelated to the text they were copying) out loud as they typed. Experiment 3 measured copy-typing performance under conditions of delayed auditory feedback, which is known to disrupt fluent speech.


## Overview of present study


# Experiment 1a and 1b: Speaking words or letters during typing

The immediate goal of typing is to produce letters in the correct order during word and sentence production. Based on the idea that verbal codes are used during typing, we assumed that typists routinely verbalize, either covertly or overtly, aspects of the letters they are typing. For example, as I type this sentence, my subjective experience is that I silently voice the words I am typing. In Experiment 1a, we asked typists recruited online from Amazon's mechanical turk to copy-type a paragraph under two conditions to silently voice each word as they typed, or silently voice each letter as they typed. To gain an understanding of the subjective experience of using the inner voice during typing, we also asked typists to report what their inner voice usually says during typing (e.g., words, letters, both, or none). Because we had no way of controlling whether our subjects adopted our instructions, we conducted Experiment 1b as a lab-based replication and extension of Experiment 1a. Here, subjects copied five paragraphs. The first paragraph involved only the instruction to copy-type as quickly and accurately as possible. Across the remaining four paragraphs we manipulated whether subjects spoke out loud or silently to themselves, combined factorially with whether they spoke words or letters during typing. If typists use verbal codes at the word level during typing, then we expected that typing speed should be faster when subjects were instructed to say words rather than letters as they typed.

## Methods

### Subjects

In experiment 1a, 50 subjects were recruited from AMT and compensated $0.25 for participating in the approximately 5 minute task. 40 subjects completed the task and were included in the analysis. In experiment 1b, 16 subjects were recruited from the undergraduate population at Brooklyn College, and received course credit for their participation. One subject was not included in the analysis because they repeatedly typed "asdf", rather than completing the task.

### Apparatus & Stimuli

For experiment 1a, typing tests were programmed for the online environment using HTML and JavaScript and conducted in subjects' web-browsers. For experiment 1b, typing tests were conducted on an iMac (21" screen) controlled by LIVECODE software. Typing responses were registered on a standard QWERTY keyboard. Each paragraph in the typing task involved copy-typing short paragraphs (~115 words length), taken from Logan and Zbrodoff (1998).

### Design and Procedure

During the task, participants were shown each paragraph in a text box. Paragraph text was black and presented in 14pt, Helvetica font. In general, participants were instructed to begin typing with the first letter in the paragraph. Correctly typed letters turned green, and typists could only proceed to the next by typing the current letter correctly. For each of the different typing conditions, subjects were presented with the following instructions. 

For experiment 1a, the instructions read: This experiment tests how you your inner voice influences your typing ability. When you type this paragraph, use your inner voice to say each word (or letter in the letter condition) that you type. At the beginning of the experiment, subjects were also asked to choose whether they use their inner voice during typing to speak either words or letters.

For experiment 1b, in the speak aloud condition, the instructions read: When you type this paragraph, speak aloud each word (or letter in the letter condition) that you type. In the inner voice condition, the instructions were identical to experiment 1a. At the beginning of the experiment, subjects were given a short questionnaire about how they use their inner voice during typing. The questionnaire stated "People have the ability to use their inner voice to speak silently to themselves. We are interested in how you use your inner voice while you are typing. Please estimate the percentage of times that your inner voice is engaged in each of the following tasks while typing, and make sure your numbers add up to 100.". The four tasks were: a) inner voice is silent, b) inner voice speaks words that you are typing, c) inner voice speaks letters that you are typing, d) inner voice speaks other words or letters that you are not typing.

## Results

```{r E1Ab, echo=FALSE, eval=FALSE}
#load E1A data
E1_data <- talk_type_E1A_data

# IKSI analysis

E1_data <- E1_data %>%
            mutate(subject = as.factor(subject)) %>%
            filter(errors == 1,
                   iksis < 5000) %>%
            group_by(subject,paragraph) %>%
            summarise(mean_iksi = mean(modified_recursive_moving(iksis)$restricted),
                      prop_removed = modified_recursive_moving(iksis)$prop_removed)
                  
E1_aov_out <- aov(mean_iksi ~ paragraph + Error(subject/paragraph), E1_data)
E1_apa_print <- apa_print(E1_aov_out)
E1_means <- model.tables(E1_aov_out,"means")

# Accuracy

E1acc_data <- talk_type_E1A_data

E1acc_data <- E1acc_data %>%
            mutate(subject = as.factor(subject)) %>%
            group_by(subject,paragraph) %>%
            summarise(mean_acc = mean(errors))
                  
E1acc_aov_out <- aov(mean_acc ~ paragraph + Error(subject/paragraph), E1acc_data)
E1acc_apa_print <- apa_print(E1acc_aov_out)
E1acc_means <- model.tables(E1acc_aov_out,"means")

E1A_dmg <- talk_type_E1A_dmg
proportion_word <- E1A_dmg %>%
                    group_by(InnerVoice) %>%
                    summarize(p_word = length(InnerVoice)/dim(E1A_dmg)[1])

inner_voice_props<-c(
  round(proportion_word[proportion_word$InnerVoice=="Words",]$p_word, digits=2),
round(proportion_word[proportion_word$InnerVoice=="Letters",]$p_word,digits=2),
round(proportion_word[proportion_word$InnerVoice=="undefined",]$p_word,digits=2))


```

```{r E1A, echo=FALSE}
load("E1A_workspace.RData")
```


For each subject, we applied the following pre-processing steps. We included IKSIs only for keystrokes involving a lower case letter, and only for correct keystrokes that were preceded by a correct keystroke. Outlier IKSIs were removed for each subject, on a cell-by-cell basis, using the @van1994solution non-recursive moving criterion procedure, which eliminated approximately `r round(mean(E1_data$prop_removed), digits=2)` of IKSIs from further analysis. An alpha criterion of .05 was set for all analyses.

### Experiment 1a

Mean IKSIs for correctly typed letters were computed for each subject in each condition and submitted to a 2 (Inner Voice: Say Letters vs. Say Words) x 2 (Letter position: First letter vs. Middle Letter) repeated measures ANOVA. Mean IKSIs are displayed in Figure \@ref(fig:E1Figure).

```{r E1Figure, echo=FALSE, fig.width=6, fig.height=3, fig.cap="Mean interkeystroke intervals (IKSIs in ms) and mean accuracy as a function of inner voice condition (Say Word vs. Say Letter), and letter position in word (first letter vs. middle letter). Middle letters were any remaining letter in the word."}
ggarrange(E1A_graph_iksi,E1A_graph_acc)
```

The main effect of inner voice condition was significant, `r E1_apa_print$full_result$linguistic_unit`. Mean IKSIs were shorter when subjects were instructed to use their inner voice to say the words (`r printnum(E1_means$tables$linguistic_unit["say_word"], digits=0)`), rather than the letters (`r printnum(E1_means$tables$linguistic_unit["say_letter"], digits=0)`) they were typing.

The main effect of letter position was significant, `r E1_apa_print$full_result$LetterType`. Mean IKSIs were longer for letters in the first position (`r printnum(E1_means$tables$LetterType["First"], digits=0)`), compared to letters in the other positions (`r printnum(E1_means$tables$LetterType["Middle"], digits=0)`).

The interaction between inner voice and letter position did not meet the significance criterion, `r E1_apa_print$full_result$linguistic_unit_LetterType`.

A corresponding 2x2 repeated measures ANOVA was conducted on the mean accuracies for typing letters in each condition. Mean accuracies are shown in Figure \@ref(fig:E1Figure). Accuracy was uniformly high in all conditions, and there were no main effects or interaction.

Last, prior to the experiments subjects were asked if their inner voice usually thinks in words or letters when they type. The proportions of subjects reporting thinking in words, letters, or who did not answer were `r paste(inner_voice_props, collapse=", ")`, respectively.

### Experiment 1b

Mean IKSIs for correctly typed letters were computed for each subject in each condition and submitted to a 2 (Speech type: out loud vs. silent) x 2 (Task: words vs. letters) repeated measures ANOVA. 

Only the main effect of task was significant, F (1, 14) = 43.38, MSE = 3775.74, p < .001. Again, mean IKSIs in the word condition (197 ms) were faster than the letter condition (301 ms). The main effect of speech type was not significant, and neither was the two-way interaction.

A corresponding analysis of error rates found only a small but significant main effect of task, F (1, 14) = 6.96, MSE = .0015, p = 0.019. Mean error rates were slightly higher in the word (.12) than letter conditions (.09).

The mean responses for the inner voice questionnaire were that subjects' inner voice was silent 3%, said words 69%, said letters 16%, or said unrelated words or letters 12% of the time during typing.

## Discussion

The main finding from both experiments was that mean typing speed was much faster when subjects covertly or overtly said the words they were typing rather than the letters they were typing. The behavioral data was also supported by the questionnaire data which showed that most subjects claim to use their inner voice to say the words they are typing rather than say letters, unrelated words or letters, or nothing at all. This finding is consistent with the idea that verbal codes at the word level are important mediators of keystroke production during typing. 

# Experiment 2: Verbal Suppression

Verbal suppression techniques are commonly used in memory studies to prevent subjects from using their inner voice to verbally code and rehearse to-be-remembered information. The technique usually involves asking subjects to repeatedly recite a word or phrase out loud during the rehearsal period. As a result, verbal processes are occupied by the reciting the word, and presumed to be unavailable for rehearsing information in working memory. Experiment 2 asked whether similar verbal suppression techniques would also disrupt typing performance. We had subjects copy-type one of six paragraphs under one normal condition, and five verbal suppression conditions. Each verbal suppression condition had typists say words or letters out loud while they typed. These conditions included: a) repeating the word "the", b) repeating the words Tuesday and Thursday, c) repeating the letters of the alphabet in order, d) repeating randomly chosen letters, and e) counting backwards from X. 

## Methods

### Subjects 

15 subjects were recruited from the undergraduate population at Brooklyn College, and received course credit for their participation. 

### Apparatus & Stimuli. 

The same apparatus and stimuli from Experiment 1b were used in Experiment 2.

### Design and Procedure. 

The same general typing test procedure from Experiment 1b was used, with the exception that typists copied six paragraphs. The instructions for each of the conditions were as follows:

	Normal Typing instructions:
	Say the instructions:
	Say Tuesday/Thursday instructions:
	Say alphabet instructions:
	Say random letters instructions:
	Count instructions:
	
## Results

Mean IKSIs for correctly typed letters were computed for each subject in each condition and submitted to a one-way repeated measures ANOVA with Suppression condition (Normal, The, Tuesday/Thursday, Alphabet, Random, and Count) as the sole factor. Mean IKSIs in each condition are displayed in table 2. 

The main effect of Suppression was significant, F () = , MSE  = , p < . Post-hoc tests showed that all mean IKSIs in all suppression conditions were significantly slower than the mean IKSI in the normal typing condition. Additionally, X was X etc.



# General Discussion

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
